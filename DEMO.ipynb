{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0DFpJpnUiOF"
      },
      "source": [
        "# DEMO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqSL7X2bQUq3"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers accelerate sentencepiece gradio numpy scipy librosa unidecode inflect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZXHY4qhK4BX",
        "outputId": "a89be943-37ca-49a9-e0e0-b1a2c88a0cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Found existing installation: torch 2.1.0+cu118\n",
            "Uninstalling torch-2.1.0+cu118:\n",
            "  Successfully uninstalled torch-2.1.0+cu118\n",
            "Found existing installation: torchvision 0.16.0+cu118\n",
            "Uninstalling torchvision-0.16.0+cu118:\n",
            "  Successfully uninstalled torchvision-0.16.0+cu118\n",
            "Found existing installation: numpy 1.24.4\n",
            "Uninstalling numpy-1.24.4:\n",
            "  Successfully uninstalled numpy-1.24.4\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.0+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
            "Collecting torchvision==0.16.0+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (2025.3.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (2.1.0)\n",
            "Collecting numpy (from torchvision==0.16.0+cu118)\n",
            "  Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu118) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu118) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0+cu118) (1.3.0)\n",
            "Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "Installing collected packages: numpy, torch, torchvision\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.1.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.1.2 torch-2.1.0+cu118 torchvision-0.16.0+cu118\n",
            "Collecting numpy==1.24.4\n",
            "  Using cached numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.2\n",
            "    Uninstalling numpy-2.1.2:\n",
            "      Successfully uninstalled numpy-2.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.3.3 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "cb8b6f9e50734c5b8f7ac37e9f3d70bf",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "# Clean up broken/incompatible versions\n",
        "!pip uninstall torch torchvision numpy -y\n",
        "\n",
        "# Install PyTorch >= 2.1 with CUDA 11.8\n",
        "!pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Downgrade NumPy to a compatible version\n",
        "!pip install numpy==1.24.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AYS7WL_4X0Ea",
        "outputId": "436f7bcb-fd2f-4fd5-8e89-bb454b223224"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-okkanLRV1M"
      },
      "outputs": [],
      "source": [
        "# Hugging Face auth\n",
        "hf_token = \"xxxxx\"\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BQZyi7aTkw5",
        "outputId": "cfbfa555-494e-4cf3-ca25-e9e6074d6f54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ],
      "source": [
        "# Load LLaMA-3.2-1B-Instruct\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 300,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.3,\n",
        "    \"do_sample\": True,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6ygA0h1U2Dv",
        "outputId": "f51534e6-619a-4758-9379-ae8c91cd2c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt7QrngZU8s8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "# Load the RAG dataset from Hugging Face\n",
        "dataset = load_dataset(\"neural-bridge/rag-dataset-12000\")\n",
        "\n",
        "train_set = dataset[\"train\"]  # Train split\n",
        "test_set = dataset[\"test\"]  # Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn1Hg6vWU_d7",
        "outputId": "cf8ef0e7-783a-4dad-c64e-63a98c177c16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original size train: 9600\n",
            "Size train after removing NaNs: 9598\n",
            "\n",
            "Original size test: 2400\n",
            "Size train after removing NaNs: 2399\n"
          ]
        }
      ],
      "source": [
        "# Remove examples where any field is empty or just whitespace from both train set and test set\n",
        "print(f\"Original size train: {len(train_set)}\")\n",
        "train_set = train_set.filter(lambda example: bool(example[\"context\"] is not None and example[\"context\"].strip()))\n",
        "train_set = train_set.filter(lambda example: bool(example[\"question\"] is not None and example[\"question\"].strip()))\n",
        "train_set = train_set.filter(lambda example: bool(example[\"answer\"] is not None and example[\"answer\"].strip()))\n",
        "print(f\"Size train after removing NaNs: {len(train_set)}\\n\")\n",
        "\n",
        "print(f\"Original size test: {len(test_set)}\")\n",
        "test_set = test_set.filter(lambda example: bool(example[\"context\"] is not None and example[\"context\"].strip()))\n",
        "test_set = test_set.filter(lambda example: bool(example[\"question\"] is not None and example[\"question\"].strip()))\n",
        "test_set = test_set.filter(lambda example: bool(example[\"answer\"] is not None and example[\"answer\"].strip()))\n",
        "print(f\"Size train after removing NaNs: {len(test_set)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s423JzUjVDKU"
      },
      "outputs": [],
      "source": [
        "full_set = concatenate_datasets([train_set, test_set])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFABXE-fQVan",
        "outputId": "69c834a8-b876-4565-972f-7a57ed00da57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
            "<ipython-input-28-d3cfbab3eb1b>:84: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Chat History\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ac4b5ae79c441efb8f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://ac4b5ae79c441efb8f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mock document corpus\n",
        "doc_corpus = full_set[\"context\"]\n",
        "\n",
        "# Realistic simple retriever\n",
        "class DocRetrieveRerank:\n",
        "    def __init__(self, docs, semb_model=None, xenc_model=None, device='cuda'):\n",
        "        self.docs = docs\n",
        "\n",
        "    def retrieve(self, query, n_docs=2):\n",
        "        query_words = set(query.lower().split())\n",
        "        ranked = sorted(self.docs, key=lambda doc: len(query_words & set(doc.lower().split())), reverse=True)\n",
        "        return ranked[:n_docs], None\n",
        "\n",
        "context_retriever = DocRetrieveRerank(docs=doc_corpus)\n",
        "\n",
        "# Response generators\n",
        "def generate_response(system_prompt, user_prompt, pipe, generation_args):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "    output = pipe(messages, **generation_args)\n",
        "    return output[0]['generated_text']\n",
        "\n",
        "def get_response(query, pipe, generation_args):\n",
        "    system_prompt = \"You are a friendly chatbot. You answer user questions.\"\n",
        "    user_prompt = f\"Answer the user question: {query}\"\n",
        "    return generate_response(system_prompt, user_prompt, pipe, generation_args)\n",
        "\n",
        "def get_rag_response(query, context_retriever, pipe, generation_args):\n",
        "    n_docs = 2\n",
        "    most_rel_docs, _ = context_retriever.retrieve(query, n_docs)\n",
        "    system_prompt = (\n",
        "        \"You are a friendly chatbot. You answer user questions based on the question and the context documents you are provided with.\"\n",
        "    )\n",
        "    user_prompt = (\n",
        "        f\"Answer the user question: {query}\\n\\n\"\n",
        "        f\"Here are some context documents that could be useful to answer:\\n```\\n{chr(10).join(most_rel_docs)}\\n```\"\n",
        "    )\n",
        "    return generate_response(system_prompt, user_prompt, pipe, generation_args)\n",
        "\n",
        "# Load Tacotron2 + WaveGlow from TorchHub\n",
        "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16', trust_repo=True).to('cuda').eval()\n",
        "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp16', trust_repo=True).to('cuda').eval()\n",
        "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils', trust_repo=True)\n",
        "\n",
        "# Remove weight norm (only once at startup)\n",
        "from torch.nn.utils import remove_weight_norm\n",
        "for layer in waveglow.modules():\n",
        "    try:\n",
        "        remove_weight_norm(layer)\n",
        "    except Exception:\n",
        "        pass\n",
        "waveglow = waveglow.half()\n",
        "\n",
        "# Real TTS synthesis\n",
        "def synthesize_tts(text):\n",
        "    sequences, lengths = utils.prepare_input_sequence([text])\n",
        "    sequences = sequences.to('cuda')\n",
        "    lengths = lengths.to('cuda')\n",
        "    with torch.no_grad():\n",
        "        mel, _, _ = tacotron2.infer(sequences, lengths)\n",
        "        mel = mel.half()\n",
        "        audio = waveglow.infer(mel)\n",
        "    return (22050, audio[0].data.cpu().numpy())\n",
        "\n",
        "# Gradio UI\n",
        "def chat_with_rag(user_input, chat_history):\n",
        "    try:\n",
        "        standard = get_response(user_input, pipe, generation_args)\n",
        "        rag = get_rag_response(user_input, context_retriever, pipe, generation_args)\n",
        "        chat_history = chat_history or []\n",
        "        chat_history.append((\"You\", user_input))\n",
        "        chat_history.append((\"Bot\", f\" Standard:\\n{standard}\\n\\n RAG:\\n{rag}\"))\n",
        "        audio = synthesize_tts(rag)\n",
        "        return chat_history, audio\n",
        "    except Exception as e:\n",
        "        chat_history.append((\"System\", f\"⚠️ Error: {e}\"))\n",
        "        return chat_history, None\n",
        "\n",
        "# Launch UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Voice Bot\")\n",
        "    chatbot = gr.Chatbot(label=\"Chat History\")\n",
        "    user_input = gr.Textbox(label=\"Ask me something:\")\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "    audio_output = gr.Audio(label=\"RAG Answer\")\n",
        "    chat_state = gr.State([])\n",
        "\n",
        "    submit_btn.click(fn=chat_with_rag,\n",
        "                     inputs=[user_input, chat_state],\n",
        "                     outputs=[chatbot, audio_output])\n",
        "\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
